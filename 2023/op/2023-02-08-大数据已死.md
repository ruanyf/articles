## 本周话题：大数据已死

“大数据”这个词，大家想必耳熟能详。这是最大众化的 IT 词汇之一，全社会曾经都热衷于它。

![](https://cdn.beekka.com/blogimg/asset/202302/bg2023022203.webp)

[百度指数](https://index.baidu.com/v2/main/index.html#/trend/%E5%A4%A7%E6%95%B0%E6%8D%AE?words=%E5%A4%A7%E6%95%B0%E6%8D%AE)显示，“大数据”从2011年开始进入搜索引擎，然后快速传播，在2017年～2019年之间达到顶峰。

那时，大家都认为，数据在未来将指数式增长，世界将被海量数据淹没。如何处理这些数据，就成为了关键问题。它决定了一个企业甚至一个国家在信息时代的竞争力。

于是，企业纷纷寻求大数据解决方案，出现了很多相关的招聘岗位，还都是高薪。高校也积极响应，有[报道](https://m.mp.oeeee.com/a/BAAFRD000020200728349302.html)称，国内有600多所高校开设了“大数据专业”或“大数据学院”，其中包括[北京大学](https://www.math.pku.edu.cn/bks/zyjs/69751.htm)、[复旦大学](https://sds.fudan.edu.cn/)这样的名校。

![](https://cdn.beekka.com/blogimg/asset/202302/bg2023022204.webp)

但是，十年过去了，大数据并没有成为发展的瓶颈，我们依然足以处理产生的所有数据，在可预见的将来也是如此。

**预言中的大数据时代，看上去不仅没有来临，反而变得遥遥无期了。** “大数据“这个词的热度，也在不断降温，被提及次数变少了，招聘岗位也慢慢不见了。

与之相应的是，”大数据“这个技术领域，也进展甚微，没有诞生新的概念和理论，技术没有突破，很多方向都停滞不前。

比如，专为处理大数据而设计的 NoSQL 数据库，声势越来越小，陷入停滞，反而是传统的关系型数据库（SQLite、Postgres、MySQL）强劲增长，越发受欢迎。

这是怎么回事？

![](https://cdn.beekka.com/blogimg/asset/202302/bg2023022808.webp)

谷歌的大数据工程师乔丹·蒂加尼 （Jordan Tigani），最近直言不讳地说：“[大数据已死](https://motherduck.com/blog/big-data-is-dead/)”。

他认为，**大数据时代已经结束了，大数据的存储和分析，作为一个技术问题已经解决了。** 用户已经不必担心数据大小了，再多的数据都不是问题。

他提出了“大数据已死”的6个理由，我觉得很有说服力，下面就跟大家分享。

<u>（1）绝大多数企业到不了大数据级别。</u> 企业的数据量往往不到 1TB，很多甚至不到 100GB。

假设一家中等规模的制造业公司，拥有1000个客户，每个客户每天产生一个订单，每个订单包含100个产品。这家公司一天产生的数据量，依然远远小于 1 MB。三年后，数据总量也只有 1 GB，达到 1 TB 需要几千年。

就算是大型互联网公司，大多数时候也到不了大数据级别。假设某个营销活动有100万用户参加，并且同一时间，该公司开展了几十个这样的营销活动，每天的数据量依然不足 1 GB，就算加上各种日志，可能也只有几个 GB，这跟大数据相差甚远。

<u>（2）存储和计算正在分离。</u>  大数据包含“数据存储”和“数据计算”两个方面，如果放在一个系统里面处理，确实很难。

但是，这两方面现在已经能够脱钩，变成两个独立系统，各自都能独立扩展。这意味着，“数据计算”不受“数据存储”（数据库大小）的限制，反之依然。

因此，大数据作为单一问题就不存在了，变成了海量存储和大型计算两个问题。

<u>（3）没有新业务的情况下，数据是线性增长的，</u> 即每天的新增数据与以前的数据结构相同。

以前的数据一旦写入数据库，通常就不再发生变化，也没有新的计算需求，相关计算在以前就完成了。这时只要对最近的新增数据进行单独计算，然后保存就可以了。你很少需要每天扫描一遍旧数据：那些数据一成不变，为什么要一遍一遍计算它们呢？

因此，对于一家企业来说，“数据会指数式增长”这个假设并不成立。而且，数据计算的需求，其实比数据存储的需求小得多，因为老数据很少需要再次计算。

<u>（4）人们看重的往往只是最近的数据</u>。最频繁的查询是针对24小时内产生的数据，一周前的数据的查询可能性要低20倍，一个月前的历史数据只会偶尔被查询。

这意味着，大数据更像静态数据，而不完全是动态数据。既然以前的数据很少用到，那么就可以压缩保存。一个包含10年数据的表格，可能会达到 PB 级别，但是如果压缩保存历史数据，压缩后可能不到 50 GB。

<u>（5）真正拥有大数据的公司，几乎从不查询全部数据。</u>他们90%的查询涉及的数据少于 100 MB，涉及 TB 级别数据的查询非常少。

就算查询 TB 级别数据，查询性能的优先级往往并不高。等一个周末或几天才拿到结果，通常是可以接受的。

另外，大型数据集的查询非常昂贵。谷歌的 BigQuery 的 PB 级别查询报价是 5,000 美元，即使是大公司也不会经常使用。

<u>（6）硬件的飞速发展，使得单台计算机的计算能力大增。</u> 2004年，谷歌发表 MapReduce 论文时，单机的计算能力还比较弱，很多计算必须通过分布式完成。

2006年，AWS 推出了 EC2 云主机，你只能用到一个单核 CPU 和 2 GB 内存。今天，AWS 的标准实例具有64个内核和 256 GB 内存。如果愿意多花钱，还可以拿到445个内核和超过 24 TB 内存。

单机计算能力大大增强，意味着大数据的最大难点——分布式计算——即使被用到，困难程度也大大降低。

综上所述，结论就是：**数据量已经不需要特别关注了，再也不必担心处理不了海量数据了。** 大数据作为一个技术问题，已经解决了。
